{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02394783-4ea0-4622-af1d-56b0b9f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7425ef1-6b58-4195-bf59-88980daa215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Steps\n",
    "'''Missing Values:\n",
    "The breast cancer dataset from sklearn does not contain missing values, so we don't need to handle any missing data.\n",
    "Feature Scaling:\n",
    "Scaling is essential for algorithms like SVM and k-NN, which are sensitive to the scale of the input features. We will use StandardScaler to standardize the features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e479bfb-9f7a-4262-a0dd-5a5ce9f18200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd3152-57aa-41b8-a2e6-8e483b9e7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Justification for Preprocessing\n",
    "Handling Missing Values: While this dataset has no missing values, it's a good practice to check for them in any dataset, as they can significantly affect model performance.\n",
    "Feature Scaling: Necessary for models that rely on distance metrics (like k-NN and SVM) to ensure that all features contribute equally to the distance computation.\n",
    "Step 2: Classification Algorithm Implementation\n",
    "We will implement the following algorithms:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "A statistical method for binary classification that models the probability of a class label based on one or more predictor variables.\n",
    "Suitable due to its simplicity and interpretability.\n",
    "Decision Tree Classifier\n",
    "\n",
    "A non-parametric method that splits the dataset into subsets based on feature values. It creates a tree-like model of decisions.\n",
    "Suitable for its interpretability and ability to handle both numerical and categorical data.\n",
    "Random Forest Classifier\n",
    "\n",
    "An ensemble method that constructs multiple decision trees and merges them to improve accuracy and control overfitting.\n",
    "Suitable for its robustness and ability to generalize well on unseen data.\n",
    "Support Vector Machine (SVM)\n",
    "\n",
    "A powerful classifier that finds the optimal hyperplane that separates different classes in a high-dimensional space.\n",
    "Suitable for its effectiveness in high-dimensional spaces.\n",
    "k-Nearest Neighbors (k-NN)\n",
    "\n",
    "A simple algorithm that classifies a data point based on how its neighbors are classified.\n",
    "Suitable for its simplicity and effectiveness in cases with a well-defined decision boundary.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c55a5-f343-4e64-a320-cc8116d97a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLIMENTATION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[model_name] = accuracy\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd9700-460f-442a-8daf-2f7bf6be07b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
